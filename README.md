# ğŸ§  Functional & Topic Word Detection NLP Project

## ğŸ¯ Project Overview
This project builds an **NLP system** that analyzes an English paragraph and:
1. **Detects functional words** (logical connectors such as *however*, *therefore*, *in contrast*, etc.).
2. **Classifies content words** into **semantic topics** (e.g., *Health*, *Fashion*, *Media*, *Technology*, *Environment*, *Sociology*).

The goal is to train a **medium-sized model (~3â€“5k sentences, ~50k tokens)** that generalizes well and can be **deployed as a web app** for educational and linguistic purposes.

---

## ğŸ§© Core Objectives
- Build a **custom labeled dataset** for functional and topic-based vocabulary.
- Train and evaluate ML/NLP models to identify functional vs. content tokens.
- Classify content words into 5â€“6 semantic topics.
- Deploy the model via **FastAPI + React/Next.js** with colored highlights in text.

---

## âš™ï¸ Tech Stack

| Component | Technology |
|------------|-------------|
| **Language** | Python 3.10+, TypeScript (for web) |
| **NLP Libraries** | spaCy, scikit-learn, HuggingFace Transformers |
| **Data Handling** | pandas, numpy, json, csv |
| **Modeling** | RandomForest / LogisticRegression (baseline), DistilBERT (advanced) |
| **Evaluation** | seqeval, sklearn.metrics, matplotlib, seaborn |
| **Backend (API)** | FastAPI |
| **Frontend (UI)** | Next.js (React) + Tailwind CSS |
| **Deployment** | Render / Vercel / HuggingFace Spaces |
| **Version Control** | Git + GitHub |
| **Containerization (optional)** | Docker |

---

## ğŸš€ MVP Scope

### Input
A short paragraph or article (English text).

### Output
JSON response and web highlights:
```json
{
  "functional_words": ["However", "Therefore"],
  "content_topics": {
    "Health": ["body", "disorder"],
    "Fashion": ["model", "waistline"],
    "Media": ["magazine"]
  }
}
```

### Minimal Features
- Detect functional connectors with â‰¥0.85 F1.
- Identify content words and assign to 1 of 5â€“6 topics.
- REST endpoint `/analyze` that returns structured JSON.
- Web interface highlighting tokens by color (green = functional, red = content).

---

## ğŸ§± System Architecture & Pipeline

```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Text Sources          â”‚
                â”‚  (Wikipedia, BBC, IELTS)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 [1] Data Preparation
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Tokenization (spaCy)            â”‚
            â”‚ POS tagging + Lemmatization      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 [2] Auto Labeling
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ FUNC detection via lexicon       â”‚
            â”‚ CONTENT tagging via POS filteringâ”‚
            â”‚ Topic assignment via seed list   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 [3] Dataset (CSV / CoNLL)
                             â”‚
                 [4] Model Training
                             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Functional Detector (Token)â”‚ Topic Classifier (Sentence)â”‚
      â”‚ RandomForest / DistilBERT  â”‚ TF-IDF + LogisticReg.     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 [5] Evaluation
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Token-F1 / Span-F1 (seqeval)    â”‚
            â”‚ Topic Macro-F1 (sklearn)        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 [6] Deployment
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ FastAPI REST API (model inference)      â”‚
        â”‚ React/Next.js UI (text highlighting)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Dataset Plan

| Type | Size | Description |
|------|------|--------------|
| **Raw Corpus** | 3kâ€“5k sentences | Academic + general texts |
| **Tokens** | ~50k | Token-level labeled data |
| **Labels** | FUNC / CONTENT / OTHER + topic tag |
| **Topics** | Health, Fashion, Media, Tech, Environment, Sociology |
| **Format** | CSV / CoNLL (token, pos, lemma, prev, next, label, topic) |

---

## ğŸ§  Model Details

### Functional Word Detector
- **Input:** token features (`word`, `lemma`, `pos`, `prev`, `next`)  
- **Baseline:** RandomForestClassifier  
- **Advanced:** `DistilBERTForTokenClassification`  
- **Output:** FUNC / CONTENT / OTHER  

### Topic Classifier
- **Input:** sentence or paragraph of content words  
- **Model:** TF-IDF + LogisticRegression (multi-class)  
- **Output:** topic label for content tokens  

---

## ğŸ“ˆ Evaluation Metrics

| Task | Metric | Target |
|------|---------|--------|
| Functional Detection | Token-F1 / Span-F1 | â‰¥ 0.85 |
| Topic Classification | Macro-F1 | â‰¥ 0.70 |
| Runtime | < 1s per paragraph | âœ… real-time ready |

---

## ğŸŒ Deployment Plan

**Backend:**  
- FastAPI endpoint `/analyze`  
  - Loads trained models (`joblib` / `from_pretrained`)  
  - Returns JSON with token types and topics  

**Frontend:**  
- Next.js + Tailwind UI  
  - Textarea input  
  - Color-coded highlights  
  - Sidebar summary (topic counts, list of functional connectors)

**Hosting:**  
- API â†’ Render / HuggingFace Space  
- Frontend â†’ Vercel  
- Optional: Dockerfile for local deployment

---

## ğŸ§® Project Folder Structure

```
nlp-functional-topic/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_texts/
â”‚   â”œâ”€â”€ tokens_labeled.csv
â”‚
â”œâ”€â”€ lexicons/
â”‚   â”œâ”€â”€ functional_list.txt
â”‚   â”œâ”€â”€ topic_seed.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_func_model.py
â”‚   â”œâ”€â”€ train_topic_model.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ api_server.py   # FastAPI
â”‚
â”œâ”€â”€ web/               # Next.js frontend
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ components/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Model_Eval.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ—“ï¸ Suggested Timeline (6 Weeks)

| Week | Goal | Deliverables |
|------|------|--------------|
| 1 | Data collection + spaCy preprocessing | Tokenized corpus |
| 2 | Auto-label + manual cleanup | Labeled CSV |
| 3 | Train FUNC model (baseline + eval) | F1 report |
| 4 | Train Topic model + cross-check | Topic F1 report |
| 5 | Build FastAPI + Next.js UI | Working local app |
| 6 | Deploy & write documentation | Online demo + README |

---

## ğŸ“ Expected Outcomes
- A working **web app** that detects logical connectors and classifies content words by topic.  
- A **medium-sized labeled dataset (publicly reusable)**.  
- A complete ML pipeline that demonstrates practical NLP engineering skills.  
- A **strong portfolio project** for internships or junior AI/NLP roles.

---

## ğŸ§¾ License
All code and generated texts are released under MIT License.  
Model weights and dataset are safe for educational and non-commercial use.

---
